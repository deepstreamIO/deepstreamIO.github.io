"use strict";(self.webpackChunkdeepstreamio_github_io=self.webpackChunkdeepstreamio_github_io||[]).push([[7787],{2921:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/files/steps-b2deeed52eff18694816590614bd17cc.png"},5680:(e,a,t)=>{t.d(a,{xA:()=>d,yg:()=>u});var n=t(6540);function i(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){i(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,i=function(e,a){if(null==e)return{};var t,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=n.createContext({}),c=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},d=function(e){var a=c(e.components);return n.createElement(l.Provider,{value:a},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},g=n.forwardRef((function(e,a){var t=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),p=c(t),g=i,u=p["".concat(l,".").concat(g)]||p[g]||m[g]||r;return t?n.createElement(u,o(o({ref:a},d),{},{components:t})):n.createElement(u,o({ref:a},d))}));function u(e,a){var t=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=g;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[p]="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=t[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}g.displayName="MDXCreateElement"},6317:(e,a,t)=>{t.r(a),t.d(a,{contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var n=t(8168),i=(t(6540),t(5680));const r={title:"WebRTC 04: Video Editing / Canvas Streams",description:"Applying filters to a WebRTC video stream before transmitting it",tags:["WebRTC","Canvas","getUserMedia","captureStream","filter","video manipulation"]},o=void 0,s={unversionedId:"guides/webrtc/webrtc-video-manipulation",id:"guides/webrtc/webrtc-video-manipulation",title:"WebRTC 04: Video Editing / Canvas Streams",description:"Applying filters to a WebRTC video stream before transmitting it",source:"@site/docs/20-guides/webrtc/40-webrtc-video-manipulation.md",sourceDirName:"20-guides/webrtc",slug:"/guides/webrtc/webrtc-video-manipulation",permalink:"/docs/guides/webrtc/webrtc-video-manipulation",editUrl:"https://github.com/deepstreamIO/deepstreamIO.github.io/docs/20-guides/webrtc/40-webrtc-video-manipulation.md",tags:[{label:"WebRTC",permalink:"/docs/tags/web-rtc"},{label:"Canvas",permalink:"/docs/tags/canvas"},{label:"getUserMedia",permalink:"/docs/tags/get-user-media"},{label:"captureStream",permalink:"/docs/tags/capture-stream"},{label:"filter",permalink:"/docs/tags/filter"},{label:"video manipulation",permalink:"/docs/tags/video-manipulation"}],version:"current",sidebarPosition:40,frontMatter:{title:"WebRTC 04: Video Editing / Canvas Streams",description:"Applying filters to a WebRTC video stream before transmitting it",tags:["WebRTC","Canvas","getUserMedia","captureStream","filter","video manipulation"]},sidebar:"tutorialSidebar",previous:{title:"WebRTC 03:Audio & Video",permalink:"/docs/guides/webrtc/webrtc-audio-video"},next:{title:"WebRTC 05: Screen Sharing",permalink:"/docs/guides/webrtc/webrtc-screen-sharing"}},l=[{value:"How it works",id:"how-it-works",children:[],level:2},{value:"1. / 2. Capturing a webcam stream",id:"1--2-capturing-a-webcam-stream",children:[],level:2},{value:"3. Painting a video to a canvas",id:"3-painting-a-video-to-a-canvas",children:[],level:2},{value:"4. Manipulating the videostream",id:"4-manipulating-the-videostream",children:[],level:2},{value:"5. Turn the manipulated video data into a stream",id:"5-turn-the-manipulated-video-data-into-a-stream",children:[],level:2},{value:"6. Receiving the video on the other client",id:"6-receiving-the-video-on-the-other-client",children:[],level:2}],c={toc:l},d="wrapper";function p(e){let{components:a,...r}=e;return(0,i.yg)(d,(0,n.A)({},c,r,{components:a,mdxType:"MDXLayout"}),(0,i.yg)("p",null,"In the ",(0,i.yg)("a",{parentName:"p",href:"webrtc-audio-video"},"previous tutorial")," we've discussed how to share unaltered audio and video streams between browsers - but in times of Snapchat, dog snout overlays and vintage effect filters this might not be enough. So in this tutorial we'll look into manipulating the video before sending."),(0,i.yg)("h2",{id:"how-it-works"},"How it works"),(0,i.yg)("p",null,(0,i.yg)("a",{target:"_blank",href:t(2921).A},"Steps")),(0,i.yg)("p",null,"Manipulating videos takes a few steps:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Capturing a webcam stream using ",(0,i.yg)("inlineCode",{parentName:"li"},"navigator.getUserMedia()")),(0,i.yg)("li",{parentName:"ol"},"Playing the stream on an HTML5 video element"),(0,i.yg)("li",{parentName:"ol"},"Painting each frame of this video onto an HTML5 Canvas Element"),(0,i.yg)("li",{parentName:"ol"},"Reading the pixels from the first canvas on every frame, manipulating them and drawing them onto a second canvas element"),(0,i.yg)("li",{parentName:"ol"},"Using the second canvas' ",(0,i.yg)("inlineCode",{parentName:"li"},".captureStream()")," method to create a video stream and transmit it via our peer connection"),(0,i.yg)("li",{parentName:"ol"},"Receiving the manipulated stream on the other peer and playing it on a video tag")),(0,i.yg)("p",null,"Let's go trough these steps one by one. You can find the complete ",(0,i.yg)("a",{parentName:"p",href:"https://github.com/deepstreamIO/dsh-demo-webrtc-examples/blob/master/04-canvas-streams/canvas-streams.js"},"code on GitHub")," or in the editable example below."),(0,i.yg)("h2",{id:"1--2-capturing-a-webcam-stream"},"1. / 2. Capturing a webcam stream"),(0,i.yg)("p",null,"Capturing a webcam stream is done using ",(0,i.yg)("inlineCode",{parentName:"p"},"navigator.getUserMedia()")," with a constraints object specifying what we need (only video in this case) and callbacks for success and error."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-javascript"},"navigator.getUserMedia({ video: true },\n    stream => {\n        localStream = stream;\n        $( '.local video' ).attr( 'src', URL.createObjectURL( stream ) );\n        drawToCanvas();\n    },\n    error => {\n        alert( 'error while accessing usermedia ' + error.toString() );\n    }\n);\n")),(0,i.yg)("p",null,"Once the stream becomes available, we'll play it on a video tag using ",(0,i.yg)("inlineCode",{parentName:"p"},"URL.createObjectURL( stream )")," as well as drawing it onto the first canvas element."),(0,i.yg)("h2",{id:"3-painting-a-video-to-a-canvas"},"3. Painting a video to a canvas"),(0,i.yg)("p",null,"A ",(0,i.yg)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial"},"canvas")," is an HTML5 element that can be used to draw pixel data. Our video is just that - a source of pixel data that changes on every drawn frame."),(0,i.yg)("p",null,"To draw it onto a canvas we start by creating a canvas element with the same size as the video..."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-html"},'<video autoplay width="300" height="225"></video>\n<canvas width="300" height="225"></canvas>\n')),(0,i.yg)("p",null,"...and acquire a 2d-drawing context from it."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-javascript"},"var localVideo = $( '.local video' )[ 0 ];\nvar inputCtx = $( '.input-canvas canvas' )[ 0 ].getContext( '2d' );\n")),(0,i.yg)("p",null,"Now in our ",(0,i.yg)("inlineCode",{parentName:"p"},"drawToCanvas()")," method we paint every frame of the video onto the canvas using ",(0,i.yg)("inlineCode",{parentName:"p"},".drawImage()"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-javascript"},"function drawToCanvas() {\n    // draw the current frame of localVideo onto the canvas,\n    // starting at 0, 0 (top-left corner) and covering its full\n    // width and heigth\n    inputCtx.drawImage( localVideo, 0, 0, 300, 225 );\n\n    //repeat this every time a new frame becomes available using\n    //the browser's build-in requestAnimationFrame method\n    requestAnimationFrame( drawToCanvas );\n}\n")),(0,i.yg)("h2",{id:"4-manipulating-the-videostream"},"4. Manipulating the videostream"),(0,i.yg)("p",null,"At this point you should see two identical video-clips - one on a video tag, one on a canvas element. Now that our video is on a canvas, we can access its raw pixel-data using ",(0,i.yg)("inlineCode",{parentName:"p"},"ctx.getImageData()"),", manipulate it and draw it onto the second canvas. We'll do this in ",(0,i.yg)("inlineCode",{parentName:"p"},"drawToCanvas()")," on every frame to apply a simple greyscale filter:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-javascript"},"    // get the pixel data from input canvas\n    var pixelData = inputCtx.getImageData( 0, 0, 300, 255 );\n\n    var avg, i;\n\n    // apply a  simple greyscale transformation\n    for( i = 0; i < pixelData.data.length; i += 4 ) {\n        avg = (\n            pixelData.data[ i ] +\n            pixelData.data[ i + 1 ] +\n            pixelData.data[ i + 2 ]\n        ) / 3;\n        pixelData.data[ i ] = avg;\n        pixelData.data[ i + 1 ] = avg;\n        pixelData.data[ i + 2 ] = avg;\n    }\n\n    // write the manipulated pixel data to the second canvas\n    outputCtx.putImageData( pixelData, 0, 0 );\n")),(0,i.yg)("h2",{id:"5-turn-the-manipulated-video-data-into-a-stream"},"5. Turn the manipulated video data into a stream"),(0,i.yg)("p",null,"The only thing left at this point is to establish a ",(0,i.yg)("a",{parentName:"p",href:"webrtc-datachannels"},"simple peer connection")," and provide a video stream from our output canvas using its ",(0,i.yg)("inlineCode",{parentName:"p"},".captureStream()")," method."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-javascript"},"const p2pConnection = new SimplePeer({\n    initiator: document.location.hash === '#initiator',\n    stream: $( '.output-canvas canvas' )[ 0 ].captureStream()\n});\n")),(0,i.yg)("h2",{id:"6-receiving-the-video-on-the-other-client"},"6. Receiving the video on the other client"),(0,i.yg)("p",null,"All the other client has to do now is to subscribe to its connection's regular ",(0,i.yg)("inlineCode",{parentName:"p"},"'stream'")," event and display the stream on a video tag:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-javascript"},"p2pConnection.on( 'stream', remoteStream => {\n    $( '.remote video' )\n        .attr( 'src', URL.createObjectURL( remoteStream ) );\n});\n")),(0,i.yg)("p",null,"To summarize: mediastreams can not only be created from videos, but from a number of different resources such as audio elements, canvas or - as we'll see in ",(0,i.yg)("a",{parentName:"p",href:"webrtc-screen-sharing"},"the next tutorial")," - the entire browser window."))}p.isMDXComponent=!0}}]);