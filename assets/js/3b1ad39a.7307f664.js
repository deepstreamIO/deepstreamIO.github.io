"use strict";(self.webpackChunkdeepstreamio_github_io=self.webpackChunkdeepstreamio_github_io||[]).push([[1887],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return m}});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=c(a),m=i,v=u["".concat(l,".").concat(m)]||u[m]||p[m]||r;return a?n.createElement(v,o(o({ref:t},d),{},{components:a})):n.createElement(v,o({ref:t},d))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},6741:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return d},default:function(){return u}});var n=a(7462),i=a(3366),r=(a(7294),a(3905)),o=["components"],s={title:"WebRTC 04: Video Editing / Canvas Streams",description:"Applying filters to a WebRTC video stream before transmitting it",tags:["WebRTC","Canvas","getUserMedia","captureStream","filter","video manipulation"]},l=void 0,c={unversionedId:"guides/webrtc/webrtc-video-manipulation",id:"guides/webrtc/webrtc-video-manipulation",title:"WebRTC 04: Video Editing / Canvas Streams",description:"Applying filters to a WebRTC video stream before transmitting it",source:"@site/docs/20-guides/webrtc/40-webrtc-video-manipulation.md",sourceDirName:"20-guides/webrtc",slug:"/guides/webrtc/webrtc-video-manipulation",permalink:"/docs/guides/webrtc/webrtc-video-manipulation",editUrl:"https://github.com/deepstreamIO/deepstreamIO.github.io/docs/20-guides/webrtc/40-webrtc-video-manipulation.md",tags:[{label:"WebRTC",permalink:"/docs/tags/web-rtc"},{label:"Canvas",permalink:"/docs/tags/canvas"},{label:"getUserMedia",permalink:"/docs/tags/get-user-media"},{label:"captureStream",permalink:"/docs/tags/capture-stream"},{label:"filter",permalink:"/docs/tags/filter"},{label:"video manipulation",permalink:"/docs/tags/video-manipulation"}],version:"current",sidebarPosition:40,frontMatter:{title:"WebRTC 04: Video Editing / Canvas Streams",description:"Applying filters to a WebRTC video stream before transmitting it",tags:["WebRTC","Canvas","getUserMedia","captureStream","filter","video manipulation"]},sidebar:"tutorialSidebar",previous:{title:"WebRTC 03:Audio & Video",permalink:"/docs/guides/webrtc/webrtc-audio-video"},next:{title:"WebRTC 05: Screen Sharing",permalink:"/docs/guides/webrtc/webrtc-screen-sharing"}},d=[{value:"How it works",id:"how-it-works",children:[],level:2},{value:"1. / 2. Capturing a webcam stream",id:"1--2-capturing-a-webcam-stream",children:[],level:2},{value:"3. Painting a video to a canvas",id:"3-painting-a-video-to-a-canvas",children:[],level:2},{value:"4. Manipulating the videostream",id:"4-manipulating-the-videostream",children:[],level:2},{value:"5. Turn the manipulated video data into a stream",id:"5-turn-the-manipulated-video-data-into-a-stream",children:[],level:2},{value:"6. Receiving the video on the other client",id:"6-receiving-the-video-on-the-other-client",children:[],level:2}],p={toc:d};function u(e){var t=e.components,s=(0,i.Z)(e,o);return(0,r.kt)("wrapper",(0,n.Z)({},p,s,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"In the ",(0,r.kt)("a",{parentName:"p",href:"webrtc-audio-video"},"previous tutorial")," we've discussed how to share unaltered audio and video streams between browsers - but in times of Snapchat, dog snout overlays and vintage effect filters this might not be enough. So in this tutorial we'll look into manipulating the video before sending."),(0,r.kt)("h2",{id:"how-it-works"},"How it works"),(0,r.kt)("p",null,(0,r.kt)("a",{target:"_blank",href:a(6708).Z},"Steps")),(0,r.kt)("p",null,"Manipulating videos takes a few steps:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Capturing a webcam stream using ",(0,r.kt)("inlineCode",{parentName:"li"},"navigator.getUserMedia()")),(0,r.kt)("li",{parentName:"ol"},"Playing the stream on an HTML5 video element"),(0,r.kt)("li",{parentName:"ol"},"Painting each frame of this video onto an HTML5 Canvas Element"),(0,r.kt)("li",{parentName:"ol"},"Reading the pixels from the first canvas on every frame, manipulating them and drawing them onto a second canvas element"),(0,r.kt)("li",{parentName:"ol"},"Using the second canvas' ",(0,r.kt)("inlineCode",{parentName:"li"},".captureStream()")," method to create a video stream and transmit it via our peer connection"),(0,r.kt)("li",{parentName:"ol"},"Receiving the manipulated stream on the other peer and playing it on a video tag")),(0,r.kt)("p",null,"Let's go trough these steps one by one. You can find the complete ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/deepstreamIO/dsh-demo-webrtc-examples/blob/master/04-canvas-streams/canvas-streams.js"},"code on GitHub")," or in the editable example below."),(0,r.kt)("h2",{id:"1--2-capturing-a-webcam-stream"},"1. / 2. Capturing a webcam stream"),(0,r.kt)("p",null,"Capturing a webcam stream is done using ",(0,r.kt)("inlineCode",{parentName:"p"},"navigator.getUserMedia()")," with a constraints object specifying what we need (only video in this case) and callbacks for success and error."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-javascript"},"navigator.getUserMedia({ video: true },\n    stream => {\n        localStream = stream;\n        $( '.local video' ).attr( 'src', URL.createObjectURL( stream ) );\n        drawToCanvas();\n    },\n    error => {\n        alert( 'error while accessing usermedia ' + error.toString() );\n    }\n);\n")),(0,r.kt)("p",null,"Once the stream becomes available, we'll play it on a video tag using ",(0,r.kt)("inlineCode",{parentName:"p"},"URL.createObjectURL( stream )")," as well as drawing it onto the first canvas element."),(0,r.kt)("h2",{id:"3-painting-a-video-to-a-canvas"},"3. Painting a video to a canvas"),(0,r.kt)("p",null,"A ",(0,r.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial"},"canvas")," is an HTML5 element that can be used to draw pixel data. Our video is just that - a source of pixel data that changes on every drawn frame."),(0,r.kt)("p",null,"To draw it onto a canvas we start by creating a canvas element with the same size as the video..."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-html"},'<video autoplay width="300" height="225"></video>\n<canvas width="300" height="225"></canvas>\n')),(0,r.kt)("p",null,"...and acquire a 2d-drawing context from it."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-javascript"},"var localVideo = $( '.local video' )[ 0 ];\nvar inputCtx = $( '.input-canvas canvas' )[ 0 ].getContext( '2d' );\n")),(0,r.kt)("p",null,"Now in our ",(0,r.kt)("inlineCode",{parentName:"p"},"drawToCanvas()")," method we paint every frame of the video onto the canvas using ",(0,r.kt)("inlineCode",{parentName:"p"},".drawImage()"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-javascript"},"function drawToCanvas() {\n    // draw the current frame of localVideo onto the canvas,\n    // starting at 0, 0 (top-left corner) and covering its full\n    // width and heigth\n    inputCtx.drawImage( localVideo, 0, 0, 300, 225 );\n\n    //repeat this every time a new frame becomes available using\n    //the browser's build-in requestAnimationFrame method\n    requestAnimationFrame( drawToCanvas );\n}\n")),(0,r.kt)("h2",{id:"4-manipulating-the-videostream"},"4. Manipulating the videostream"),(0,r.kt)("p",null,"At this point you should see two identical video-clips - one on a video tag, one on a canvas element. Now that our video is on a canvas, we can access its raw pixel-data using ",(0,r.kt)("inlineCode",{parentName:"p"},"ctx.getImageData()"),", manipulate it and draw it onto the second canvas. We'll do this in ",(0,r.kt)("inlineCode",{parentName:"p"},"drawToCanvas()")," on every frame to apply a simple greyscale filter:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-javascript"},"    // get the pixel data from input canvas\n    var pixelData = inputCtx.getImageData( 0, 0, 300, 255 );\n\n    var avg, i;\n\n    // apply a  simple greyscale transformation\n    for( i = 0; i < pixelData.data.length; i += 4 ) {\n        avg = (\n            pixelData.data[ i ] +\n            pixelData.data[ i + 1 ] +\n            pixelData.data[ i + 2 ]\n        ) / 3;\n        pixelData.data[ i ] = avg;\n        pixelData.data[ i + 1 ] = avg;\n        pixelData.data[ i + 2 ] = avg;\n    }\n\n    // write the manipulated pixel data to the second canvas\n    outputCtx.putImageData( pixelData, 0, 0 );\n")),(0,r.kt)("h2",{id:"5-turn-the-manipulated-video-data-into-a-stream"},"5. Turn the manipulated video data into a stream"),(0,r.kt)("p",null,"The only thing left at this point is to establish a ",(0,r.kt)("a",{parentName:"p",href:"webrtc-datachannels"},"simple peer connection")," and provide a video stream from our output canvas using its ",(0,r.kt)("inlineCode",{parentName:"p"},".captureStream()")," method."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-javascript"},"const p2pConnection = new SimplePeer({\n    initiator: document.location.hash === '#initiator',\n    stream: $( '.output-canvas canvas' )[ 0 ].captureStream()\n});\n")),(0,r.kt)("h2",{id:"6-receiving-the-video-on-the-other-client"},"6. Receiving the video on the other client"),(0,r.kt)("p",null,"All the other client has to do now is to subscribe to its connection's regular ",(0,r.kt)("inlineCode",{parentName:"p"},"'stream'")," event and display the stream on a video tag:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-javascript"},"p2pConnection.on( 'stream', remoteStream => {\n    $( '.remote video' )\n        .attr( 'src', URL.createObjectURL( remoteStream ) );\n});\n")),(0,r.kt)("p",null,"To summarize: mediastreams can not only be created from videos, but from a number of different resources such as audio elements, canvas or - as we'll see in ",(0,r.kt)("a",{parentName:"p",href:"webrtc-screen-sharing"},"the next tutorial")," - the entire browser window."))}u.isMDXComponent=!0},6708:function(e,t,a){t.Z=a.p+"assets/files/steps-b2deeed52eff18694816590614bd17cc.png"}}]);